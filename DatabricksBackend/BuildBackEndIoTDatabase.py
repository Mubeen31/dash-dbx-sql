# Databricks notebook source
# MAGIC %md
# MAGIC 
# MAGIC # This notebook generates a full data pipeline from databricks dataset - iot-stream
# MAGIC 
# MAGIC ## This creates 2 tables: 
# MAGIC 
# MAGIC <b> Database: </b> plotly_iot_dashboard
# MAGIC 
# MAGIC <b> Tables: </b> silver_sensors, silver_users 
# MAGIC 
# MAGIC <b> Params: </b> StartOver (Yes/No) - allows user to truncate and reload pipeline

# COMMAND ----------

from pyspark.sql.functions import *
from pyspark.sql.types import *

# COMMAND ----------

# MAGIC %sql
# MAGIC 
# MAGIC CREATE DATABASE IF NOT EXISTS plotly_iot_dashboard;
# MAGIC USE plotly_iot_dashboard;

# COMMAND ----------

dbutils.widgets.dropdown("StartOver", "Yes", ["Yes", "No"])
startOver = dbutils.widgets.get("StartOver")
print(f"Starting from scratch: {startOver}")

# COMMAND ----------

if startOver == "Yes":
  spark.sql("TRUNCATE TABLE plotly_iot_dashboard.silver_sensors")
  spark.sql("TRUNCATE TABLE plotly_iot_dashboard.silver_users")
  
  dbutils.fs.rm(checkpoint_stream_location_device, recurse=True)
  dbutils.fs.rm(checkpoint_stream_location_user, recurse=True)

# COMMAND ----------

checkpoint_stream_location_device = f"/FileStore/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/iot_pipeline/checkpoints/device/"
auto_loader_schema_location_device = f"/FileStore/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/iot_pipeline/schemas/device/"

checkpoint_stream_location_user = f"/FileStore/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/iot_pipeline/checkpoints/user/"
auto_loader_schema_location_user = f"/FileStore/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/iot_pipeline/schemas/user/"

print(f"Streaming pipeline with checkpoint location: {checkpoint_stream_location_device}")

# COMMAND ----------

# DBTITLE 1,Read Incrementally via Streaming
df_devices = (spark
              .readStream
              .format("cloudFiles")
              .option("cloudFiles.format", "json")
              .option("cloudFiles.inferSchema", "true")
              .option("cloudFiles.schemaLocation", auto_loader_schema_location_device)
              .load("/databricks-datasets/iot-stream/data-device/")
              .withColumn("input_file_name", input_file_name())
              .select("id", "device_id", "user_id", "calories_burnt", "miles_walked", "num_steps", "timestamp", "input_file_name", "value")
             )

#display(df_devices)

# COMMAND ----------

# DBTITLE 1,Write Stream
(df_devices
 .writeStream
 .trigger(once=True)
 .option("checkpointLocation", checkpoint_stream_location_device)
 .table("plotly_iot_dashboard.bronze_sensors")
)

# COMMAND ----------

# DBTITLE 1,Create Silver Table for upserting updates
spark.sql("""
CREATE TABLE IF NOT EXISTS plotly_iot_dashboard.silver_sensors
(
Id BIGINT GENERATED BY DEFAULT AS IDENTITY,
device_id INT,
user_id INT,
calories_burnt DECIMAL(10,2), 
miles_walked DECIMAL(10,2), 
num_steps DECIMAL(10,2), 
timestamp TIMESTAMP, 
input_file_name STRING, 
value STRING
)
USING DELTA 
TBLPROPERTIES("delta.targetFileSize"="128mb")
--LOCATION s3://<path>/
""")

# COMMAND ----------

# DBTITLE 1,Perform Upserts - Device Data
# MAGIC %sql
# MAGIC 
# MAGIC MERGE INTO plotly_iot_dashboard.silver_sensors AS target
# MAGIC USING (SELECT Id::integer,
# MAGIC               device_id::integer,
# MAGIC               user_id::integer,
# MAGIC               calories_burnt::decimal,
# MAGIC               miles_walked::decimal,
# MAGIC               num_steps::decimal,
# MAGIC               timestamp::timestamp,
# MAGIC               input_file_name::string,
# MAGIC               value::string
# MAGIC               FROM plotly_iot_dashboard.bronze_sensors) AS source
# MAGIC ON source.Id = target.Id
# MAGIC AND source.user_id = target.user_id
# MAGIC AND source.device_id = target.device_id
# MAGIC WHEN MATCHED THEN UPDATE SET 
# MAGIC   target.calories_burnt = source.calories_burnt,
# MAGIC   target.miles_walked = source.miles_walked,
# MAGIC   target.num_steps = source.num_steps,
# MAGIC   target.timestamp = source.timestamp
# MAGIC WHEN NOT MATCHED THEN INSERT *;

# COMMAND ----------

# DBTITLE 1,Table Optimizations
spark.sql("""OPTIMIZE plotly_iot_dashboard.silver_sensors ZORDER BY (user_id, device_id, timestamp)""")

# COMMAND ----------

# MAGIC %md 
# MAGIC 
# MAGIC ## Ingest User Data As Well

# COMMAND ----------

df_users = (spark
              .readStream
              .format("cloudFiles")
              .option("cloudFiles.format", "csv")
              .option("cloudFiles.inferSchema", "true")
              .option("cloudFiles.schemaLocation", auto_loader_schema_location_user)
              .load("/databricks-datasets/iot-stream/data-user/")
              .withColumn("input_file_name", input_file_name())
             )

#display(df_users)

# COMMAND ----------

# DBTITLE 1,Write stream to Users Bronze Tables
(df_users
 .withColumn("update_timestamp", current_timestamp())
 .writeStream
 .trigger(once=True)
 .option("checkpointLocation", checkpoint_stream_location_user)
 .option("mergeSchema", "true")
 .table("plotly_iot_dashboard.bronze_users")
)

# COMMAND ----------

spark.sql("""
CREATE TABLE IF NOT EXISTS plotly_iot_dashboard.silver_users
(
userid BIGINT GENERATED BY DEFAULT AS IDENTITY,
gender STRING,
age INT,
height DECIMAL(10,2), 
weight DECIMAL(10,2),
smoker STRING,
familyhistory STRING,
cholestlevs STRING,
bp STRING,
risk DECIMAL(10,2),
input_file_name STRING,
update_timestamp TIMESTAMP
)
USING DELTA 
TBLPROPERTIES("delta.targetFileSize"="128mb")
--LOCATION s3://<path>/
""")

# COMMAND ----------

# MAGIC %sql
# MAGIC 
# MAGIC MERGE INTO plotly_iot_dashboard.silver_users AS target
# MAGIC USING (SELECT 
# MAGIC       userid::int,
# MAGIC       gender::string,
# MAGIC       age::int,
# MAGIC       height::decimal, 
# MAGIC       weight::decimal,
# MAGIC       smoker,
# MAGIC       familyhistory,
# MAGIC       cholestlevs,
# MAGIC       bp,
# MAGIC       risk,
# MAGIC       input_file_name,
# MAGIC       update_timestamp
# MAGIC       FROM plotly_iot_dashboard.bronze_users) AS source
# MAGIC ON source.userid = target.userid
# MAGIC WHEN MATCHED THEN UPDATE SET 
# MAGIC   target.gender = source.gender,
# MAGIC       target.age = source.age,
# MAGIC       target.height = source.height, 
# MAGIC       target.weight = source.weight,
# MAGIC       target.smoker = source.smoker,
# MAGIC       target.familyhistory = source.familyhistory,
# MAGIC       target.cholestlevs = source.cholestlevs,
# MAGIC       target.bp = source.bp,
# MAGIC       target.risk = source.risk,
# MAGIC       target.input_file_name = source.input_file_name,
# MAGIC       target.update_timestamp = source.update_timestamp
# MAGIC WHEN NOT MATCHED THEN INSERT *;

# COMMAND ----------

#spark.sql("""TRUNCATE TABLE plotly_iot_dashboard.bronze_users""")

# COMMAND ----------

# MAGIC %sql
# MAGIC 
# MAGIC SELECT * FROM plotly_iot_dashboard.silver_users;

# COMMAND ----------

# MAGIC %sql
# MAGIC 
# MAGIC SELECT * FROM plotly_iot_dashboard.silver_sensors;
